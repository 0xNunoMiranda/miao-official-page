// Configura√ß√£o centralizada para gera√ß√£o de texto
// Otimizado para m√°xima escalabilidade e economia

export interface TextGenerationConfig {
  maxLength: number
  temperature: number
  topP: number
  model: string
}

// Configura√ß√µes otimizadas por tipo de uso
export const TEXT_GENERATION_CONFIGS = {
  // Chat/Conversa - Respostas curtas e r√°pidas (otimizado para escalabilidade)
  CHAT: {
    maxLength: 20, // Reduzido de 32 para 20 - respostas mais curtas, mais r√°pido, mais escal√°vel
    temperature: 0.6, // Ligeiramente mais criativo
    topP: 0.85,
    model: "gpt-neo-125M", // Modelo mais leve
  } as TextGenerationConfig,

  // Meme/Conte√∫do criativo - Um pouco mais de tokens para criatividade
  CREATIVE: {
    maxLength: 30, // Reduzido de 50 para 30 - ainda criativo, mas mais eficiente
    temperature: 0.7,
    topP: 0.9,
    model: "gpt-neo-125M",
  } as TextGenerationConfig,

  // Conte√∫do mais longo quando necess√°rio
  LONG_FORM: {
    maxLength: 50,
    temperature: 0.5,
    topP: 0.8,
    model: "gpt-neo-125M",
  } as TextGenerationConfig,
} as const

// Configura√ß√£o padr√£o (mais econ√¥mica)
export const DEFAULT_CONFIG = TEXT_GENERATION_CONFIGS.CHAT

/**
 * BENEF√çCIOS DE REDUZIR TOKENS:
 * 
 * 1. VELOCIDADE ‚ö°
 *    - Menos tokens = menos tempo de processamento
 *    - Respostas instant√¢neas melhoram UX
 * 
 * 2. ESCALABILIDADE üìà
 *    - Mais requests dentro dos mesmos limites de rate
 *    - Exemplo: 32 tokens vs 20 tokens = 60% mais requests poss√≠veis
 * 
 * 3. DISPONIBILIDADE DE WORKERS üë∑
 *    - Workers preferem requests menores (mais r√°pido de processar)
 *    - Requests menores t√™m mais chance de serem aceitos
 *    - Menos "No available workers" errors
 * 
 * 4. ECONOMIA üí∞
 *    - Menos computa√ß√£o = menos recursos consumidos
 *    - Se usar APIs pagas no futuro, custo proporcionalmente menor
 * 
 * 5. CONFIABILIDADE ‚úÖ
 *    - Requests menores s√£o mais confi√°veis
 *    - Menos timeouts e falhas
 * 
 * 6. HUGGING FACE FREE TIER üÜì
 *    - Com 1000 req/dia: 20 tokens = 50k tokens/dia vs 32 tokens = 32k tokens/dia
 *    - 56% mais capacidade com tokens menores!
 * 
 * EXEMPLO DE IMPACTO:
 * - Antes: 32 tokens √ó 1000 req = 32.000 tokens/dia
 * - Agora:  20 tokens √ó 1000 req = 20.000 tokens/dia (mas 60% mais requests cabem)
 * - Em teoria: 20 tokens √ó 1600 req = 32.000 tokens/dia (mesmo total, mais requests!)
 */
